# Bigram Language Model (Character-Level) in PyTorch

A simple yet powerful character-level Bigram Language Model that learns from names and generates realistic new ones using either statistical bigrams or a small neural network.

---

## Features

- Character-level modeling (a–z and `.` for start/end)
- Bigram count matrix with visualization
- Neural network trained on bigram transitions
- Sampling for new name generation
- Negative Log-Likelihood (NLL) evaluation
- Clean PyTorch implementation

---

## Directory Structure

```
.
├── N-gram Language Model(Bigram).py  # Main model script
├── names.txt                         # Training data
└── README.md                         # This file
```

---

## Dataset

The file `names.txt` should contain a list of names, one per line. Example:

```
emma
oliver
ava
liam
```

---

## Requirements

Install required Python packages:

```bash
pip install torch matplotlib
```

---

## How to Run

Ensure `names.txt` is in the working directory. Then run:

```bash
python "N-gram Language Model(Bigram).py"
```

---

## How the Model Works

### Step-by-Step Overview

1. Preprocessing  
   - Names are prepended and appended with `.` as start/end markers.
   - All unique characters are mapped to indices.

2. Bigram Counting  
   - Counts of `(ch1, ch2)` pairs are stored in a 27×27 matrix `N`.

3. Probability Estimation  
   - Convert counts to probabilities using row-wise normalization.

4. Log-Likelihood Computation  
   - Model quality is evaluated using negative log-likelihood (NLL).

5. Neural Model (Bigram Net)  
   - A simple 1-layer network is trained using one-hot encoding.
   - Loss = NLL + L2 Regularization.

6. Sampling  
   - New names are generated by sampling from the model until the end token is reached.

---

## Example Output

After training, the model may generate names like:

```
milo
sarina
don.
lia
```

---

## Bigram Matrix Visualization

The script will generate a heatmap of the bigram frequency matrix using `matplotlib`.  
Each cell represents how often character `j` follows character `i`.

---

## Training Loss

The loss used during neural network training is:

```python
loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()
```

- `probs`: softmax-normalized prediction
- `ys`: ground truth next character index
- Regularization ensures smoother learning

---

## Sampling New Names

Sampling is done one character at a time:

```python
ix = 0  # start from '.'
while True:
    probs = softmax(xenc @ W)
    ix = sample_next_index(probs)
    if ix == 0: break  # stop at '.'
    out.append(itos[ix])
```

---

## Evaluation: Negative Log-Likelihood (NLL)

The model maximizes the log-likelihood of real name sequences:

```python
log_likelihood = sum(log(P[next_char | prev_char]))
nll = -log_likelihood
```

Lower `nll` means better model fit.

---

## Extensions & Ideas

- Upgrade to Trigram or N-gram models
- Integrate LSTM or Transformer layers
- Evaluate perplexity on test/validation set
- Support multilingual character sets

---

## Author

Made with love by Raasikh Naveed 
Inspired by [Andrej Karpathy](https://github.com/karpathy)
